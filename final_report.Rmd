---
title: "Quantifying crowd size with mobile phone and Twitter data - Final Report"
author: "Ogi Moore and Connor Smith"
date: "12/5/2016"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(scales)
library(grid)
library(plyr)
library(gridExtra)
library(knitr)
```

# Introduction

We elected to replicate the findings of Federico Botta, Helen Susannah Moat, and Tobias Preis's paper on [Quantifying crowd size with mobile phone and _Twitter_ data](http://rsos.royalsocietypublishing.org/content/2/5/150162).  In the paper, they look at a number of soccer games with a known attendence and known phone, internet and twitter acitivity; and they evaluate the similar phone and internet and twitter acitivty in comparison to a number of flights over a several week period.   
# Data Sources  

## SMS, Call, and Internet Data  
Data collected for internet, phone calls, SMS, and Twitter data was provided by the Telecom Italia Big Data Challenge. The data was acquired and anonymized by Telecom Italia. The data originates from Milan and surrounding areas between 1 November 2013 and 31 December 2013.  
  
All interactions on the mobile network generate Call Detail Records (CDRs). These are acquired by the following parameters:  
- SMS Data  
    - CDR is generated for each SMS sent and recieved  
- Call Data  
    - CDR is generated for each call sent and recieved  
- Internet Access: CDR is generated for the following events:  
    - Internet connection is opened  
    - Internet Connection is closed  
    - Internet Connection is open and 15 minutes has passed since last CDR  
    - Internet Connection is open and 5 MB have been transferred since last CDR was generated  
  
After being collected, the data was rescaled for privacy reasons. The SMS and call data were scaled using the same factor, while internet data was scaled using a different factor.  
  
## Twitter Data  
Similarly to the SMS, Call, and Internet Data, geo-localised tweets were collected from the Big Data Challenge (same time and location). It is not indicated that any data rescaling was completed. The paper does not state that the Twitter data is "normalized," so it is assumed to be untouched.  
  
## Football Match Attendees  
Football match attendance was retrieved from the [Italian National Football League 'Serie A' official website](http://www.legaseriea.it/it/lega-calcio/regolamenti-e-documenti/dati-statistici-su-incassi-e-spettatori). The final three games attendance data was obtained from two online newspapers ([Calciomercato](http://www.calciomercato.com/news/inter-trapani-3-2-il-tabellino-919259), [Milan News 1](http://www.milannews.it/il-match/quasi-cinquantamila-spettatori-riempiono-san-siro-105483), and [Milan News 2](http://www.milannews.it/il-match/milan-ajax-superati-i-61mila-spettatori-a-san-siro-130840)).  
  
## Airport Data  
The airport data was retrieved from the [Linate Airport website](http://www.milanolinate-airport.com/it). This data is only available for the current date + 4 days, so the data collected was for Monday 5 May 2014 through 11 May 2014. The measures used for estimating each hour of passenger amounts was calculated by summing the number of flights departing in the next 2 hours and the number of flights arriving in the past hour.  

# Data Import

The data is in very good shape, but we do need to tell `R` that the timestamps are in-fact date-time objects, and not just generic strings.

```{r import}
san_siero.attendees = read.csv('./data/Attendees_San_Siro.csv')
san_siero.phone_data = read.csv('./data/San_Siro_Mobile_Phone_Data.csv')
san_siero.twitter_data = read.csv('./data/San_Siro_Twitter_Data.csv')
linate.data = read.csv('./data/Linate_Data.csv')
linate.flight_schedule = read.csv('./data/Linate_Flights_Schedule.csv')

# Converting to dates
san_siero.phone_data$Timestamp = as.Date(strptime(san_siero.phone_data$Timestamp, 
                                                  "%Y-%m-%d %H:%M:%S"))
san_siero.twitter_data$Timestamp = as.Date(san_siero.twitter_data$Timestamp)
san_siero.attendees$Date = as.Date(san_siero.attendees$Date)
```

# Soccer Game Attendence

The soccer data includes 3 raw files, one containing the dates of the soccer games as well as the attendence.  The phone data includes a measure of calls / SMS activity and Internet activity with a timestamp.  We will show later, that we are only interested in the date, and have no interest in the time.  The third file includes a measure of twitter activity with an associate date.  To do an analysis on this data, we need to merge the data into a common dataframe, where we are interested only in the maximum value of the calls / SMS activity and Internet activity numbers for a given day.  We then exclude days that did not have any soccer matches, and have our data.frame ready for analysis.

```{r soccer_cleanup}
san_siero.daily_data <- aggregate(san_siero.phone_data$Calls.and.SMS.Activity, 
                                  by=list(Category=san_siero.phone_data$Timestamp), 
                                  FUN=max)
san_siero.daily_data <- rename(san_siero.daily_data, c("x"="Calls.and.SMS.Activity"))
san_siero.daily_data$Internet.Activity = aggregate(san_siero.phone_data$Internet.Activity, 
                                         by=list(Cateogry=san_siero.phone_data$Timestamp), 
                                         FUN=max)$x
san_siero.daily_data$Twitter.Activity = san_siero.twitter_data$Twitter.Activity
san_siero.daily_data <- rename(san_siero.daily_data, c("Category"="Date"))
soccer_data <- merge(san_siero.daily_data, san_siero.attendees, 
                                        by="Date")

kable(head(soccer_data),
			format='pandoc',
			caption='Soccer Game Data',
			centering=TRUE)
```

First, the authors created a plot showing the various measures (calls and SMS data, internet activity, Twitter activity, and attendance) vs date. We are able to replicate this data using `R`  

```{r figure2, echo=FALSE}
p1 <- ggplot(san_siero.phone_data, aes(x=Timestamp, y=Calls.and.SMS.Activity)) + 
      geom_line(colour='#354CB0',size=1) + 
      scale_y_continuous(breaks=seq(0,600,300)) + 
      coord_cartesian(ylim=c(0,600)) +
      labs(y='Call and SMS') + 
      theme(panel.background = element_blank(),
          legend.position='none',
          axis.text.x=element_blank(),
          axis.title.x=element_blank(),
          axis.title.y=element_text(angle=90,size=8),
          axis.ticks.x=element_blank(),
          panel.border = element_rect(colour='black',fill=NA, size=1))
p2 <- ggplot(san_siero.phone_data, aes(x=Timestamp, y=Internet.Activity)) + 
      geom_line(colour='#7876C9',size=1) + 
      scale_y_continuous(breaks=seq(0,150,75)) + 
      coord_cartesian(ylim=c(0,200)) +
      labs(y='Internet') + 
      theme(panel.background = element_blank(),
          legend.position='none',
          axis.text.x=element_blank(),
          axis.title.x=element_blank(),
          axis.title.y=element_text(angle=90,size=8),
          axis.ticks.x=element_blank(),
          panel.border = element_rect(colour='black',fill=NA, size=1))
p3 <- ggplot(san_siero.twitter_data, aes(x=Timestamp, y=Twitter.Activity)) + 
      geom_line(colour='#E41D1A',linetype="dashed",size=1) + 
      scale_y_continuous(breaks=seq(0,150,75)) + 
      coord_cartesian(ylim=c(0,200)) +
      geom_point(colour='#E41D1A',shape = 15,size=4,alpha=0.5) +
      geom_point(colour='#E41D1A') +
      labs(y='Twitter') + 
      theme(panel.background = element_blank(),
          legend.position='none',
          axis.text.x=element_blank(),
          axis.title.x=element_blank(),
          axis.title.y=element_text(angle=90,size=8),
          axis.ticks.x=element_blank(),
          panel.border = element_rect(colour='black',fill=NA, size=1))
p4 <- ggplot(san_siero.attendees, aes(x=Date, y=Attendees.at.San.Siro)) + 
      geom_bar(stat='identity',colour='#959190',width=0.5) +       
      scale_y_continuous(breaks=seq(0,60000,30000)) + 
      coord_cartesian(ylim=c(0,80000)) +
      scale_x_date(labels = date_format('%d %b'),limits = c(as.Date('2013-11-01'), 
                                                            as.Date('2013-12-30')),
                   breaks=c(as.Date(san_siero.attendees$Date))) + 
      labs(y='Attendees') + 
      theme(panel.background = element_blank(),
          legend.position='none',
          axis.title.y=element_text(angle=90,size=8),
          axis.text.x=element_text(angle = 45,hjust=1),
          panel.border = element_rect(colour='black',fill=NA, size=1))

grid.draw(rbind(ggplotGrob(p1), 
                ggplotGrob(p2), 
                ggplotGrob(p3), 
                ggplotGrob(p4), size="last"))
```

This chart shows that the activity data tends to peak in relation to the attendee data.  For comparison, here is the plot attached to the paper:

\begin{center}
\includegraphics[width=0.8\textheight]{F2.large}
\end{center}

The authors then performed a linear regression comparing calls and SMS activity, Internet activity, _Twitter_ activity to the number of attendees.  With `R` we are able to perform the same linear regression analysis with ease.

```{r soccer_lm}
attendees_v_phone <- lm(soccer_data$Attendees.at.San.Siro ~ 
                          soccer_data$Calls.and.SMS.Activity)
attendees_v_internet <- lm(soccer_data$Attendees.at.San.Siro ~ 
                             soccer_data$Internet.Activity)
attendees_v_twitter <- lm(soccer_data$Attendees.at.San.Siro ~ 
                            soccer_data$Twitter.Activity)
```

Now we can compare the values we calculated vs. those in the paper.

```{r soccer_lm_results}
lm_paper_results <- c(0.771, 0.937, 0.855)
lm_duplication_results <- c(round(summary(attendees_v_phone)$adj.r.squared, 3),
                            round(summary(attendees_v_internet)$adj.r.squared, 3),
                            round(summary(attendees_v_twitter)$adj.r.squared, 3))
lm_results <- data.frame(lm_paper_results, 
                         lm_duplication_results,
                         row.names=c('Calls and SMS Data', 
                                     'Internet Activity', 
                                     'Twitter Activity'))
kable(lm_results,
			format='pandoc',
			centering=TRUE,
			caption='Linear Regression R^2^ Values',
			col.names = c('Published Results', 'Duplication Results'))
```

We can see that our $R^2$ values match up exactly.  The results show that the internet activity mobile phone data was the most accurate predictor of crowd size, having the highest R^2^ value.

Next, the authors evaluated the correlation to see how the relationship  holds up to a non-parametric analysis.  We are able to calculate the same correlation values.

```{r soccer_cor_results}
cor_paper_results <- c(0.927, 0.976, 0.924)
cor_duplication_results <- c(round(cor(soccer_data$Attendees.at.San.Siro,
                                       soccer_data$Calls.and.SMS.Activity,
                                       method='spearman'), 3),
                             round(cor(soccer_data$Attendees.at.San.Siro,
                                       soccer_data$Internet.Activity,
                                       method='spearman'), 3),
                             round(cor(soccer_data$Attendees.at.San.Siro,
                                       soccer_data$Twitter.Activity,
                                       method='spearman'), 3))
cor_results <- data.frame(cor_paper_results,
                          cor_duplication_results,
                          row.names=c('Calls and SMS Data', 
                                      'Internet Activity', 
                                      'Twitter Activity'))
kable(cor_results,
			format='pandoc',
			caption='Spearman Correlation Values',
			col.names = c('Published Results', 'Duplication Results'))
```

Our spearman correlation values match up precisely as well.  These results also indicate that Internet activity is the best guage for crowd size at the soccer games.  Since we have come to the same conclusions, we can replicate their plot.

## Linear Modeling  

Four plots were made by the authors to show the linear relationship that each of the measures of activity has with the amount of people in the area. The first three plots were very straight forward, comparing values for activity to attendance at soccer games.  The fourth plot was a little tricky.  


```{r figure_3, echo=FALSE}
# Thanks to: https://stackoverflow.com/questions/10762287/
#            how-can-i-format-axis-labels-with-exponents-with-ggplot2-and-scales
scientific_formatter <- function(x){
  output <- gsub("e", " %*% 10^", scientific_format()(x))
  output <- gsub("^+", "^", output, fixed=TRUE)
  output <- gsub("%*% 10^00", "", output, fixed=TRUE)
  formatted_output <- parse(text=output)
  return(formatted_output)
}

p1 <- ggplot(soccer_data, aes(Calls.and.SMS.Activity, 
                              Attendees.at.San.Siro)) + 
	    geom_point(col='#354CB0') +
      coord_cartesian(ylim=c(0,90000),xlim=c(0,600)) +
      scale_y_continuous(labels=scientific_formatter,
                         breaks=seq(0,80000,20000)) +
      scale_x_continuous(limits=c(-10000,100000),breaks=seq(0,600,300)) + 
      geom_smooth(method ='lm', se=FALSE, colour='#959190', fullrange=TRUE,size=2)  +
      labs(x='Calls and SMS Activity', y='Attendees') +
      theme(panel.background = element_blank(),
            panel.border = element_rect(colour='black', fill=NA, size=1),
            panel.grid.major = element_blank())
      
p2 <- ggplot(soccer_data, aes(Internet.Activity, 
                              Attendees.at.San.Siro)) +
	    geom_point(colour='#7876C9') +
      coord_cartesian(ylim=c(0,90000),xlim=c(0,200)) +
      scale_y_continuous(labels=scientific_formatter,
                         breaks=seq(0,80000,20000)) + 
      scale_x_continuous(limits=c(-10000,100000),breaks=seq(0,200,100)) + 
      geom_smooth(method ='lm', se=FALSE, colour='#959190', fullrange=TRUE,size=2)  +
      labs(x='Internet activity') +
      theme(aspect.ratio=1,
            panel.background = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank(),
            axis.title.y = element_blank(),
            panel.border = element_rect(colour='black', fill=NA, size=1))

p3 <- ggplot(soccer_data, aes(Twitter.Activity, 
                              Attendees.at.San.Siro)) + 
	    geom_point(col='#E41D1A') +
      coord_cartesian(ylim=c(0,90000),xlim=c(0,200)) +
      scale_y_continuous(labels=scientific_formatter,
                         breaks=seq(0,80000,20000)) + 
      scale_x_continuous(limits=c(-10000,100000),breaks=seq(0,200,100)) + 
      geom_smooth(method ='lm', se=FALSE, colour='#959190', fullrange=TRUE,size=2) +
      labs(x='Twitter activity') +
      theme(aspect.ratio=1,
            panel.background = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank(),
            axis.title.y = element_blank(),
            panel.border = element_rect(colour='black', fill=NA, size=1))

model <- lm(Attendees.at.San.Siro ~ Calls.and.SMS.Activity + 
                Internet.Activity +
                Twitter.Activity,
              data=soccer_data)

attendence <- data.frame(predicted = predict.lm(model, 
                                                newdata=soccer_data,
                                                interval='confidence'),
                         actual = soccer_data$Attendees.at.San.Siro)

p4 <- ggplot(attendence, aes(x=actual,y=predicted.fit)) +
          geom_errorbar(aes(ymin = predicted.lwr, ymax = predicted.upr), 
                       colour = "blue", alpha = 0.2,size=4) + 
          geom_point(colour = "blue") +
          geom_smooth(method='lm', se=FALSE, colour='#959190', fullrange=TRUE,size=2) + 
          scale_y_continuous(labels=scientific_formatter,
                             breaks=seq(10000,90000,40000),
                             position = 'right') + 
          coord_cartesian(ylim=c(0,90000), 
                          xlim=c(0,90000)) +
          scale_x_continuous(labels=scientific_formatter, 
                             limits=c(-10000,100000),
                             breaks=seq(10000,90000,40000)) + 
          labs(x='Actual', y='Predicted') +
          theme(aspect.ratio=1,
                panel.background = element_blank(),
                panel.border = element_rect(colour='black', fill=NA, size=1))

p1 <- ggplot_gtable(ggplot_build(p1))
p2 <- ggplot_gtable(ggplot_build(p2))
p3 <- ggplot_gtable(ggplot_build(p3))
p4 <- ggplot_gtable(ggplot_build(p4))

maxHeight = unit.pmax(p1$heights[2:3], p2$heights[2:3], p3$heights[2:3], p4$heights[2:3])
p1$heights[2:3] <- maxHeight
p2$heights[2:3] <- maxHeight
grid.arrange(p1, p2, p3, p4, ncol=4, nrow=1, respect=TRUE,widths=c(1.15, 1, 1, 1.3))
```

For the fourth plot, the authors said they performed a _leave-one-out cross-validation_ analysis, which we are able to do in `R`. Unfortunately we were unable to get confidence interval results based on the 'LOOCV' model method, so we then resorted to doing a generic linear model.  When we did the standard linear model (`lm`) method, we noticed the predicted numbers were the same as the predicted output from the _leave-one-out cross-validation_, but `R` provided confidence intervals.  The confidence intervals match up with the plot published in the paper.  

For comparison, here is the plot attached to the paper:

\begin{center}
\includegraphics[width=0.5\textheight]{F3.large}
\end{center}

## Conclusion

From looking at calls/sms data, internet activity and _Twitter_ activity, it appears that internet activity is the best predictor to crowd sizes, although all methods show strong linear relationships and correlations.  Also, when we compare the projected crowd size vs. the actual crowd size, and evaluate the 95% confidence interval of what the actual crowd size is based on the predicted value, we notice that the best fit curve falls within the 95% confidence interval for all the points.

# Airport Crowd Approximations

In the airport dataset the authors took a different method to approximating the crowd size.  They approximated the number of people at the airport based on the number of arriving and departing flights.  More specifically, they summed the flights departing in the two hour window following the time of interest and the number of incoming flights in the hour leading up to the time of interest, and used the result as a relative approximation of crowd size.  The raw data provides the number of flights arriving and departing the airport on an hour by hour basis over a 1 week period.

```{r airport_flight_data}
kable(head(linate.flight_schedule),
			format='pandoc',
			caption="Linate Flight Schedule Data",
			centering=TRUE)
```

The authors also provide a relative quantity of calls and SMS activity and internet activity, as well as _Twitter_ activity

```{r airport_phone_data}
kable(head(linate.data),
			format='pandoc',
			caption='Linate Phone Data',
			centering=TRUE)
```

The reader may notice here that the dates of the time-stamps do not match up (they are off by 6 months).  The authors explain that the way they compensate for this is that they line up the days of the week from the flights data, and assume that the flight schedule remains fairly consistent for each day of the week (the airport activity at any given time on a Monday in November will be similar to the same time on a Monday in May).  They excluded November 1^st^, 2^nd^, and 3^rd^, as well as December 30^th^ and 31^st^ as they were holidays.

As the authors decided to look at the number of incoming flights up to an hour before, and the number of departing flights for two hours following, this made for having to modify the raw data and doing a calculation for 'total flights'.  Furthermore, the authors then decided to average the calls and sms activity, internet activity and twitter activity associated with any given hour and weekday over the two month span.  This kind of data wrangling is outside of our skill set in `R`, however we were able to make the modifications necessary in `Python`.  Should a reviewer wish to run this python code, they will need the Pandas library installed.  The `Python` code outputs a file titled 'Linate_wrangled.csv' which we will import into `R` to generate our statistics with, the notebook is configured such that the following code cell will not be executed (in case the reviewer does not have Python with the Pandas library installed).  The python code outputs a file titled `Linate_wrangled.csv`, which we then import into `R` further along.

```{python modified_data_wrangling, eval=FALSE}
import numpy as np
import pandas as pd
import datetime as dt
linate_sched_data = pd.read_csv('./data/Linate_Flights_Schedule.csv', 
                                parse_dates=[0],
                                infer_datetime_format=True,
                                index_col=0)
linate_sched_data['Day'] = linate_sched_data.index.weekday_name
linate_sched_data['Hour'] = linate_sched_data.index.hour

linate_sched_data['Flights'] = np.roll(linate_sched_data['Departures'], -2) + \
                               np.roll(linate_sched_data['Departures'], -1) + \
                               np.roll(linate_sched_data['Arrivals'], 1)
linate_flight_data = linate_sched_data.groupby(['Day', 'Hour']).sum()        
linate_flight_data.drop(['Arrivals', 'Departures'], inplace=True, axis=1)
linate_phone_data = pd.read_csv('./data/Linate_Data.csv',
                               parse_dates=[0],
                               infer_datetime_format=True)

days_to_skip = pd.to_datetime(['2013-11-01',
                               '2013-11-02',
                               '2013-11-03',
                               '2013-12-30',
                               '2013-12-31']).date
linate_phone_data = \
    linate_phone_data[linate_phone_data['Timestamp'].dt.date.isin(days_to_skip) == False]
linate_phone_data.set_index('Timestamp', drop=True, inplace=True)
linate_phone_data['Day'] = linate_phone_data.index.weekday_name
linate_phone_data['Hour'] = linate_phone_data.index.hour
linate_avg_phone_data = pd.DataFrame(linate_phone_data.groupby(['Day', 'Hour'], 
                                                               sort=True).mean())
result = pd.concat([linate_flight_data, linate_avg_phone_data], axis=1)
result.to_csv('./data/Linate_wrangled.csv')
```

We can now import the wrangled CSV file that our python code generated and move on with our analysis.  Here is what the head of that dataframe looks like.

```{r flight_wrangle_import}
flight_data <- read.csv('./data/Linate_wrangled.csv')
kable(head(flight_data),
			format='pandoc',
			caption='Linate Flight Data Cleaned Up',
			centering=TRUE)
```


```{r flight_analysis}
lm_paper_results <- c(0.175, 0.143, 0.510)
flights_v_phone <- lm(flight_data$Flights ~ 
                      flight_data$Calls.and.SMS.Activity)
flights_v_internet <- lm(flight_data$Flights ~ 
                         flight_data$Internet.Activity)
flights_v_twitter <- lm(flight_data$Flights ~ 
                        flight_data$Twitter.Activity)


lm_duplication_results <- c(round(summary(flights_v_phone)$adj.r.squared, 3),
                            round(summary(flights_v_internet)$adj.r.squared, 3),
                            round(summary(flights_v_twitter)$adj.r.squared, 3))

lm_results <- data.frame(lm_paper_results, 
                         lm_duplication_results,
                         row.names=c('Calls and SMS Data', 
                                     'Internet Activity', 
                                     'Twitter Activity'))

kable(lm_results,
			format='pandoc',
			centering=TRUE,
			caption='Linear Regression R^2^ Values',
			col.names = c('Published Results', 'Duplication Results'))
```

After wrangling the data and analyzing, we can see that the same values are obtained. Now, we can replicate the plots.  


```{r figure4, echo=FALSE}
p1 <- ggplot(flight_data, aes(Calls.and.SMS.Activity, Flights)) + 
      geom_point(col='#354CB0') +
      geom_smooth(method ='lm', se=FALSE, colour='#959190', fullrange=TRUE,size=2) +
      scale_y_continuous(breaks=seq(5,30,5)) + 
      coord_cartesian(ylim=c(0,35),xlim=c(0,6000)) +
      scale_x_continuous(labels=scientific_formatter, limits=c(-1000,8000),
                         breaks=c(0,seq(1000,5000,2000))) + 
      labs(x='Calls and SMS Activity', y='no. flights') +
      theme(aspect.ratio=1,
            panel.background = element_blank(),
            panel.border = element_rect(colour='black', fill=NA, size=1))

p2 <- ggplot(flight_data, aes(Internet.Activity, Flights)) + 
      geom_point(colour='#7876C9') +
      labs(x='Internet activity') +
      geom_smooth(method ='lm', se=FALSE, colour='#959190', fullrange=TRUE,size=2)  +
      scale_y_continuous(breaks=seq(5,30,5)) + 
      coord_cartesian(ylim=c(0,35),xlim=c(0,15000)) +
      scale_x_continuous(labels=scientific_formatter, limits=c(-1000,20000),
                         breaks=seq(0,10000,5000)) + 
      theme(aspect.ratio=1,
            panel.background = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank(),
            axis.title.y = element_blank(),
            panel.border = element_rect(colour='black', fill=NA, size=1))

p3 <- ggplot(flight_data, aes(Twitter.Activity, Flights)) + 
      geom_point(col='#E41D1A') +
      labs(x='Twitter activity') +
      geom_smooth(method ='lm', se=FALSE, colour='#959190', fullrange=TRUE,size=2) +
      scale_y_continuous(breaks=seq(5,30,5)) + 
      coord_cartesian(ylim=c(0,35),xlim=c(0,3.5)) +
      scale_x_continuous(labels=scientific_formatter, limits=c(-2,5),
                         breaks=seq(0,3,1)) + 
      theme(aspect.ratio=1,
            panel.background = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank(),
            axis.title.y = element_blank(),
            panel.border = element_rect(colour='black', fill=NA, size=1))
grid.arrange(p1, p2, p3, ncol=3, nrow=1, respect=TRUE)
```

For comparison, here is the plot attached to the paper

\begin{center}
\includegraphics[width=0.5\textheight]{F4.large}
\end{center}

## Conclusion  

As it can be seen, our plots line up exactly with the ones published.  It should be noted that the authors of this paper made some assumptions that we do not agree with.  For one, they used flight data from a period 5-6 months after the data of cell phone activity and assumed that the flight schedule would remain consistent on a day by day schedule for the period over the phone data.  Given a lack of raw data for flights at the appropriate window, this assumption would need to be made, but it is one that could definitely be a major source of error.  The second issue is that the period of cell phone and internet activity recorded includes the Christmas holidays, which we would assume the number of passengers at the airport during this time would be different enough from outside the holiday period that it may be a source for error.  This potential source for error is minimized due to the way the grouping is done (summing all the recorded values for the same day of the week and hour of the day).