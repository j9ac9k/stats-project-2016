---
title: "Quantifying crowd size with mobile phote and Twitter data - Final Report"
author: "Ogi Moore and Connor Smith"
date: "12/5/2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(scales)
library(grid)
library(plyr)
library(gridExtra)
library(knitr)
```

# Introduction

We elected to replicate the findings of Federico Botta, Helen Susannah Moat, and Tobias Preis's paper on [Quantifying crowd size with mobile phone and _Twitter_ data](http://rsos.royalsocietypublishing.org/content/2/5/150162).  In the paper, they look at a number of soccer games with a known attendence and known phone, internet and twitter acitivity; and they evaluate the similar phone and internet and twitter acitivty in comparison to a number of flights over a several week period.

# Data Import

The data is in very good shape, but we do need to tell `R` that the timestamps are in-fact times, and not just generic strings.

```{r import}
san_siero.attendees = read.csv('./data/Attendees_San_Siro.csv')
san_siero.phone_data = read.csv('./data/San_Siro_Mobile_Phone_Data.csv')
san_siero.twitter_data = read.csv('./data/San_Siro_Twitter_Data.csv')
linate.data = read.csv('./data/Linate_Data.csv')
linate.flight_schedule = read.csv('./data/Linate_Flights_Schedule.csv')

# Converting to dates
san_siero.phone_data$Timestamp = as.Date(strptime(san_siero.phone_data$Timestamp, 
                                                  "%Y-%m-%d %H:%M:%S"))
san_siero.twitter_data$Timestamp = as.Date(san_siero.twitter_data$Timestamp)
san_siero.attendees$Date = as.Date(san_siero.attendees$Date)
```

The soccer game raw data is comprised of 3 seperate files, so we need to merge them together based on the relevant timestamps.

```{r soccer_cleanup}
san_siero.daily_data <- aggregate(san_siero.phone_data$Calls.and.SMS.Activity, 
                                  by=list(Category=san_siero.phone_data$Timestamp), 
                                  FUN=max)
san_siero.daily_data <- rename(san_siero.daily_data, c("x"="Calls.and.SMS.Activity"))
san_siero.daily_data$Internet.Activity = aggregate(san_siero.phone_data$Internet.Activity, 
                                         by=list(Cateogry=san_siero.phone_data$Timestamp), 
                                         FUN=max)$x
san_siero.daily_data$Twitter.Activity = san_siero.twitter_data$Twitter.Activity
san_siero.daily_data <- rename(san_siero.daily_data, c("Category"="Date"))
soccer_data <- merge(san_siero.daily_data, san_siero.attendees, 
                                        by="Date")

kable(head(soccer_data),
			format='pandoc',
			caption='Soccer Game Data',
			centering=TRUE)
```


# Soccer Games Dataset

The authors performed a linear regression comparing calls and SMS activity, Internet activity, _Twitter_ activity to the number of attendees.  With `R` we are able to perform the same linear regression analysis with ease.

```{r soccer_analysis}
attendees_v_phone <- lm(soccer_data$Attendees.at.San.Siro ~ soccer_data$Calls.and.SMS.Activity)
attendees_v_internet <- lm(soccer_data$Attendees.at.San.Siro ~ soccer_data$Internet.Activity)
attendees_v_twitter <- lm(soccer_data$Attendees.at.San.Siro ~ soccer_data$Twitter.Activity)

lm_paper_results <- c(0.771, 0.937, 0.855)
lm_duplication_results <- c(round(summary(attendees_v_phone)$adj.r.squared, 3), 
														round(summary(attendees_v_internet)$adj.r.squared, 3),
														round(summary(attendees_v_twitter)$adj.r.squared, 3))

lm_results <- data.frame(lm_paper_results, 
												 lm_duplication_results,
												 row.names=c('Calls and SMS Data', 
																		 'Internet Activity', 
																		 'Twitter Activity'))

cor_paper_results <- c(0.927, 0.976, 0.924)
cor_duplication_results <- c(round(cor(soccer_data$Attendees.at.San.Siro,
																 			 soccer_data$Calls.and.SMS.Activity,
																 			 method='spearman'), 3),
														 round(cor(soccer_data$Attendees.at.San.Siro,
														 		 			 soccer_data$Internet.Activity,
														 		 			 method='spearman'), 3),
														 round(cor(soccer_data$Attendees.at.San.Siro,
														 					 soccer_data$Twitter.Activity,
														 					 method='spearman'), 3))

cor_results <- data.frame(cor_paper_results,
													cor_duplication_results,
												  row.names=c('Calls and SMS Data', 
													 					  'Internet Activity', 
																		  'Twitter Activity'))
```

Now that we've done our duplication of analysis, we can compare our results to those published.

```{r soccer_results}
kable(lm_results,
			format='pandoc',
			centering=TRUE,
			caption='Linear Regression R^2^ Values',
			col.names = c('Published Results', 'Duplication Results'))

kable(cor_results,
			format='pandoc',
			caption='Spearman Correlation Values',
			col.names = c('Published Results', 'Duplication Results'))
```

Here we can see that the results we computed match up exactly with the results published.



# Airport Dataset

In the airport dataset the authors took a different method to approximating the crowd size.  They approximated the number of people at the airport based on the number of outgoing flights for two hours following a specific time, and the incoming flights for an hour leading up to a specific time.  The raw data provides the number of flights arriving and departing the airport on an hour by hour basis over a 1 week period.

```{r airport_flight_data}
kable(head(linate.flight_schedule),
			format='pandoc',
			caption="Linate Flight Schedule Data",
			centering=TRUE)
```

The authors also provide a relative quantity of calls and SMS activity and internet activity, as well as Twitter activity

```{r airport_phone_data}
kable(head(linate.data),
			format='pandoc',
			caption='Linate Phone Data',
			centering=TRUE)
```

The reader may notice here that the dates of the time-stamps do not match up (they are off by 6 months).  The authors explain that the way they compensate for this is that they line up the days of the week from the flights data, and assume that the flight schedule remains fairly consistent week for week.  They excluded November 1^st^, 2^nd^, and 3^rd^, as well as December 30^th^ and 31^st^.

As the authors decided to look at the number of incoming flights up to an hour before, and the number of departing flights for two hours following, this made for having to modify the raw data substantially.  This was outside of our skill set in `R`, however we were able to make the modifications necessary in `Python`.

```{python3 flight_data_wrangling}
import csv
import datetime
converted_data = {}
days_to_skip = set(['2013-11-01', '2013-11-02', '2013-11-03', '2013-12-31', '2013-12-30'])
with open('./data/Linate_Flights_Schedule.csv') as csvfile:
    content = csv.reader(csvfile, delimiter=',')
    next(content, None)
    list_content = list(content)
    # Calculate the total number of relevant flights at any given time
    for index, row in enumerate(list_content):
        if row[0].split(' ')[0] in days_to_skip:
            continue
        total_flights = sum([int(row[1]), int(row[2])])
        try:
            total_flights += int(list_content[index+1][1])
        except IndexError:
            pass
        output_day = datetime.datetime.strptime(row[0].split(' ')[0], 
                                                '%Y-%m-%d').strftime('%a')
        converted_data[' '.join([output_day, row[0].split(' ')[1]])] = total_flights
        
with open('./data/Linate_Data.csv') as csvfile:
    content = csv.reader(csvfile, delimiter=',')
    next(content, None)
    list_content = list(content)
    # Attach the number of flights to the mobile phone data point
    for row in list_content:
        date = row[0].split(' ')[0]
        time = row[0].split(' ')[1]
        day = datetime.datetime.strptime(date, '%Y-%m-%d').strftime('%a')
        row.append(converted_data[' '.join([day, time])])

# output the new data to a CSV file        
with open('./data/Linate_wrangled.csv', 'w') as csvfile:
    wr = csv.writer(csvfile, delimiter=',', lineterminator='\n')
    wr.writerow(['Timestamp', 
                 'Calls.and.SMS.Activity', 
                 'Internet.Activity', 
                 'Twitter.Activity', 
                 'Flights'])
    wr.writerows([row for row in list_content])
```

Here, the python file generated a new csv file that we will import with `R` to do our analysis with.

```{r flight_data_import}
linate_flight_data = read.csv('./data/Linate_wrangled.csv')
linate_flight_data$Timestamp = as.Date(strptime(linate_flight_data$Timestamp, 
                                                "%Y-%m-%d %H:%M:%S"))
kable(head(linate_flight_data),
			format='pandoc',
			caption='Linate Flight Data Cleaned Up',
			centering=TRUE)
```

### Notes from Connor:
I just noticed that the data from the flights is still in the data set after wrangling. Was this intended? If not, I think that will also help clean up data.  

I figured out the aggregation. The paper says "We compare this proxy indicator to the average mobile phone call and SMS activity recorded for each hour in a week, in the cells in which the airport is located."  

This, to me, means that we should have one data point (an average) that corresponds to each hour of the week (24 hours X 7 days), then compare that to the flight data. leave us with 168 data points on each graph instead of 1464. I will work on doing these calculations and then post my result. I am unfamiliar with some of the methods in your python script, but that is likely the best way to take care of this part.  


```{python3 modified data wrangling}
import csv
import datetime as dt
import pandas as pd

linate_sched_data = pd.read_csv('./data/Linate_Flights_Schedule.csv')
#linate_sched_data['Date'] = linate_sched_data['Timestamp'].apply(lambda x: dt.datetime.strptime(x,'%Y-%m-%d %H:%M:%S').date())
linate_phone_data = pd.read_csv('./data/Linate_Data.csv')
#linate_phone_raw['Date'] = linate_phone_raw['Timestamp'].apply(lambda x: dt.datetime.strptime(x,'%Y-%m-%d %H:%M:%S').date())

# Establish days to skip
days_to_skip_str = ['2013-11-01','2013-11-02','2013-11-03','2013-12-30','2013-12-31']
days_to_skip_dt = [dt.datetime.strptime(date,'%Y-%m-%d').date() for date in days_to_skip_str]
linate_phone_data.head()

# Remove ignored dates
for i,row in linate_phone_data.iterrows():
    if dt.datetime.strptime(row['Timestamp'],'%Y-%m-%d %H:%M:%S').date() in days_to_skip_dt:
        linate_phone_data.drop(i,inplace=True)
        
# Add columns
linate_phone_data['day'] = pd.Series('', index=linate_phone_data.index)
linate_phone_data['hour'] = pd.Series('', index=linate_phone_data.index)
linate_sched_data['day'] = pd.Series('', index=linate_sched_data.index)
linate_sched_data['hour'] = pd.Series('', index=linate_sched_data.index)
# Populate Columns
for i, row in linate_phone_data.iterrows():
    linate_phone_data.loc[i,'day'] = str(dt.datetime.strptime(row['Timestamp'],'%Y-%m-%d %H:%M:%S').weekday())
    linate_phone_data.loc[i,'hour'] = str(dt.datetime.strptime(row['Timestamp'],'%Y-%m-%d %H:%M:%S').hour)
for i, row in linate_sched_data.iterrows():
    linate_sched_data.loc[i,'day'] = str(dt.datetime.strptime(row['Timestamp'],'%Y-%m-%d %H:%M:%S').weekday())
    linate_sched_data.loc[i,'hour'] = str(dt.datetime.strptime(row['Timestamp'],'%Y-%m-%d %H:%M:%S').hour)

linate_final = pd.DataFrame(linate_phone_data.groupby(['day','hour'],sort=True).mean())
linate_final['Flights'] = pd.Series('', index=linate_final.index)
linate_sched_data['total_flights'] = pd.Series('', index=linate_sched_data.index)

#unfinished, needs more work


```



